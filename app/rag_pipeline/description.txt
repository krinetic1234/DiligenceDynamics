"""multi_modal_RAG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18jLqeBxlncP_MXrCLiPLX96W2Q_Bxzmr

## Semi-structured and Multi-modal RAG

Many documents contain a mixture of content types, including text, tables, and images.

Semi-structured data can be challenging for conventional RAG for at least two reasons:

* Text splitting may break up tables, corrupting the data in retrieval
* Embedding tables may pose challenges for semantic similarity search

And the information captured in images is typically lost.

With the emergence of multimodal LLMs, like [GPT4-V](https://openai.com/research/gpt-4v-system-card), it is worth considering how to utilize images in RAG:

`Option 1:`

* Use multimodal embeddings (such as [CLIP](https://openai.com/research/clip)) to embed images and text
* Retrieve both using similarity search
* Pass raw images and text chunks to a multimodal LLM for answer synthesis

`Option 2:`

* Use a multimodal LLM (such as [GPT4-V](https://openai.com/research/gpt-4v-system-card), [LLaVA](https://llava.hliu.cc/), or [FUYU-8b](https://www.adept.ai/blog/fuyu-8b)) to produce text summaries from images
* Embed and retrieve text
* Pass text chunks to an LLM for answer synthesis

`Option 3:`

* Use a multimodal LLM (such as [GPT4-V](https://openai.com/research/gpt-4v-system-card), [LLaVA](https://llava.hliu.cc/), or [FUYU-8b](https://www.adept.ai/blog/fuyu-8b)) to produce text summaries from images
* Embed and retrieve image summaries with a reference to the raw image
* Pass raw images and text chunks to a multimodal LLM for answer synthesis   

This cookbook show how we might tackle this :

* We will use [Unstructured](https://unstructured.io/) to parse images, text, and tables from documents (PDFs).
* We will use the [multi-vector retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector) to store raw tables, text, (optionally) images along with their summaries for retrieval.
* We will demonstrate `Option 2`, and will follow-up on the other approaches in future cookbooks.

![ss_mm_rag.png](attachment:9bbbcfe4-2b85-4e76-996a-ce8d1497d34e.png)

## Packages
"""

! pip install langchain unstructured[all-docs] pydantic lxml

!apt-get update

!sudo apt-get install poppler-utils tesseract-ocr

!pip install langchain_openai

!pip install backoff
!pip install openai --upgrade

!pip install chromadb
!pip install --upgrade sqlalchemy

!apt-get install -y cmake
!git clone https://github.com/ggerganov/llama.cpp.git

!mkdir models
!wget -O models/ggml-model-q5_k.gguf https://huggingface.co/mys/ggml_llava-v1.5-7b/resolve/main/ggml-model-q5_k.gguf?download=true
!wget -O models/mmproj-model-f16.gguf https://huggingface.co/mys/ggml_llava-v1.5-7b/resolve/main/mmproj-model-f16.gguf?download=true


Images Processing with LLaVA:
# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# cd llama.cpp
# rm -r build
# # mkdir build && cd build && cmake ..
# # cmake --build .
# ls

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# cd llama.cpp/
# make

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# # Define the directory containing the images
# 
# IMG_DIR=./figures
# ls
# 
# # Loop through each image in the directory
# for img in "${IMG_DIR}"*.jpg; do
#     # Extract the base name of the image without extension
#     base_name=$(basename "$img" .jpg)
# 
#     # Define the output file name based on the image name
#     output_file="${IMG_DIR}${base_name}.txt"
# 
#     # Execute the command and save the output to the defined output file
#     ./llama.cpp/bin/llava -m ../models/llava-7b/ggml-model-q5_k.gguf --mmproj ../models/llava-7b/mmproj-model-f16.gguf --temp 0.1 -p "Describe the image in detail. Be specific about graphs, such as bar plots." --image "$img" > "$output_file"
# 
# done
#